{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8288ef81-3b2e-4572-a176-eb1303940997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2029/3661155198.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Target Column 'Uplink_delay' appended to 'ml-results/Uplink_delay.csv'\n",
      "Results for Target Column 'Downlink_delay' appended to 'ml-results/Downlink_delay.csv'\n",
      "Results for Target Column 'Uplink_pcktloss' appended to 'ml-results/Uplink_pcktloss.csv'\n",
      "Results for Target Column 'Downlink_pcktloss' appended to 'ml-results/Downlink_pcktloss.csv'\n",
      "Results for Target Column 'Uplink_thrpt' appended to 'ml-results/Uplink_thrpt.csv'\n",
      "Results for Target Column 'Downlink_thrpt' appended to 'ml-results/Downlink_thrpt.csv'\n",
      "Results for Target Column 'sta_nrg' appended to 'ml-results/sta_nrg.csv'\n",
      "Results for Target Column 'ap_nrg' appended to 'ml-results/ap_nrg.csv'\n",
      "Results for Target Column 'sta_overhead' appended to 'ml-results/sta_overhead.csv'\n",
      "Results for Target Column 'ap_overhead' appended to 'ml-results/ap_overhead.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "#from sklearn.linear_model import Lasso\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.linear_model import Ridge\n",
    "#from sklearn.svm import SVR\n",
    "#from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time \n",
    "import os\n",
    "\n",
    "all_data = pd.read_csv (\"combined_data.csv\")\n",
    "\n",
    "# Packets per second is disabled for full buffer traffic\n",
    "all_data.loc[all_data[\"Generated_traffic\"] == \"Full Buffer\", \"Packets_per_second\"] = int(100.0)\n",
    "target_columns = [\n",
    "    \"Uplink_delay\", \"Downlink_delay\", \"Uplink_pcktloss\", \"Downlink_pcktloss\",\n",
    "    \"Uplink_thrpt\", \"Downlink_thrpt\", \"sta_nrg\", \"ap_nrg\", \"sta_overhead\", \"ap_overhead\"\n",
    "]\n",
    "scenario_list = [\"Communication_link\", \"Power_save_mechanism\", \"Generated_traffic\" ,\"Transport_protocol\" ,\"Packets_per_second\" ,\"Sta_count\"]\n",
    "all_data[target_columns] = all_data[target_columns].fillna(0)\n",
    "scenario_set = all_data[scenario_list]\n",
    "numerical_columns= [\"Packets_per_second\", \"Sta_count\"]\n",
    "categorical_columns= [\"Communication_link\", \"Power_save_mechanism\", \"Generated_traffic\", \"Transport_protocol\"]\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown= \"ignore\")\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer (\n",
    "        [\n",
    "            (\"one_hot_encoder\", categorical_preprocessor, categorical_columns),\n",
    "            (\"standard_scaler\", numerical_preprocessor, numerical_columns),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model = make_pipeline (preprocessor, GradientBoostingRegressor(random_state=42))\n",
    "\n",
    "param_grid = {    \n",
    "#    'decisiontreeregressor__max_depth': [3, 5, 10, None],\n",
    "#    'decisiontreeregressor__min_samples_split': [2, 5, 10],\n",
    "#    'decisiontreeregressor__min_samples_leaf': [1, 2, 4]\n",
    "    'gradientboostingregressor__n_estimators': [100, 200, 300],\n",
    "    'gradientboostingregressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gradientboostingregressor__max_depth': [3, 5, 7],\n",
    "    'gradientboostingregressor__min_samples_split': [2, 5, 10],\n",
    "    'gradientboostingregressor__min_samples_leaf': [1, 2, 4]\n",
    "#    'ridge__alpha': [0.1, 1, 10, 100, 1000]\n",
    "#    'lasso__alpha': [0.1, 1, 10, 100, 1000]\n",
    "#    'randomforestregressor__n_estimators': [100, 200, 300],\n",
    "#    'randomforestregressor__max_depth': [None, 10, 20, 30],\n",
    "#    'randomforestregressor__min_samples_split': [2, 5, 10],\n",
    "#    'randomforestregressor__min_samples_leaf': [1, 2, 4]\n",
    "#    'svr__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#    'svr__C': [0.1, 1, 10, 100],\n",
    "#    'svr__epsilon': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Initialize results list\n",
    "results_dict = {}\n",
    "for target_column in target_columns:\n",
    "    target = all_data[target_column].fillna(0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scenario_set, target, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Use GridSearchCV to search for the best hyperparameter with cross-validation\n",
    "    train_time_start= time.time()\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    train_time_end= time.time()\n",
    "\n",
    "    # Perform cross-validation on the best model\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    cv_mse_scores = -cv_scores  # Convert to positive mean squared error\n",
    "    cv_rmse_scores = np.sqrt(cv_mse_scores)\n",
    "    # Make predictions\n",
    "    predict_time_start = time.time()\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    predict_time_end = time.time()\n",
    "     # Collect results\n",
    "    results_df = pd.DataFrame({\n",
    "        'ML model': ['Gradient Boosting Regression'],\n",
    "        'Mean cross-validated MSE': [np.mean(cv_mse_scores)],\n",
    "        'Mean cross-validated RMSE': [np.mean(cv_rmse_scores)],\n",
    "        'Best model hyperparameters': [str(grid_search.best_params_)],\n",
    "        'model MSE': [mean_squared_error(y_test, y_pred)],\n",
    "        'model RÂ² score': [r2_score(y_test, y_pred)],\n",
    "        'model Training Time': [train_time_end - train_time_start],\n",
    "        'model Prediction Time': [predict_time_end - predict_time_start]\n",
    "    })\n",
    "    # Store results DataFrame in the dictionary\n",
    "    results_dict[target_column] = results_df\n",
    "\n",
    "results_dir = 'ml-results/'\n",
    "# Output the results DataFrames to CSV files\n",
    "for target_column, df in results_dict.items():\n",
    "    file_path = os.path.join(results_dir, f\"{target_column}.csv\")    \n",
    "    # Check if file exists\n",
    "    if os.path.exists(file_path):\n",
    "        # Read existing data\n",
    "        existing_df = pd.read_csv(file_path)\n",
    "        # Append new results to existing data\n",
    "        combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "        # Save combined data back to file without index\n",
    "        combined_df.to_csv(file_path, index=False)\n",
    "        print(f\"Results for Target Column '{target_column}' appended to '{file_path}'\")\n",
    "    else:\n",
    "        # If file does not exist, save new results to new file without index\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Results for Target Column '{target_column}' saved to '{file_path}'\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be32220-ae98-4581-8da3-708dfe596f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
